---
title: å¤§è¯­è¨€æ¨¡å‹åŸºç¡€
description: Transformeræ¶æ„ä¸å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹
---

# å¤§è¯­è¨€æ¨¡å‹åŸºç¡€

## Transformer æ¶æ„

### æ³¨æ„åŠ›æœºåˆ¶

#### è‡ªæ³¨æ„åŠ›åŸç†
```python
# æŸ¥è¯¢ã€é”®ã€å€¼è®¡ç®—
Q = X Ã— WQ
K = X Ã— WK  
V = X Ã— WV

# æ³¨æ„åŠ›æƒé‡
Attention(Q, K, V) = softmax(QK^T / âˆšdk) V

å…¶ä¸­ï¼š
- dk: é”®å‘é‡ç»´åº¦
- softmax: å½’ä¸€åŒ–å‡½æ•°
```

#### å¤šå¤´æ³¨æ„åŠ›
```python
# å¤šå¤´è®¡ç®—
MultiHead(Q, K, V) = Concat(head1, ..., headh)WO
where headi = Attention(QWQi, KWKi, VWVi)

ç‰¹ç‚¹ï¼š
- å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›
- æ•è·ä¸åŒç±»å‹å…³ç³»
- æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›
```

### ä½ç½®ç¼–ç 

#### æ­£å¼¦ä½ç½®ç¼–ç 
```python
PE(pos, 2i) = sin(pos / 10000^(2i/dmodel))
PE(pos, 2i+1) = cos(pos / 10000^(2i/dmodel))

ç‰¹ç‚¹ï¼š
- ç»å¯¹ä½ç½®ä¿¡æ¯
- å‘¨æœŸæ€§å‡½æ•°
- å¤–æ¨æ€§æœ‰é™
```

#### å¯å­¦ä¹ ä½ç½®ç¼–ç 
```python
# å‚æ•°åŒ–ä½ç½®åµŒå…¥
position_embeddings = Embedding(max_seq_len, dmodel)

ç‰¹ç‚¹ï¼š
- çµæ´»å­¦ä¹ ä½ç½®å…³ç³»
- éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®
- å¯èƒ½è¿‡æ‹Ÿåˆ
```

### ç¼–ç å™¨-è§£ç å™¨ç»“æ„

#### ç¼–ç å™¨å±‚
```python
class EncoderLayer:
    def __init__(self):
        self.self_attn = MultiHeadAttention()
        self.ffn = FeedForward()
        self.norm1 = LayerNorm()
        self.norm2 = LayerNorm()
    
    def forward(self, x):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        x = self.norm1(x + self.self_attn(x, x, x))
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥  
        x = self.norm2(x + self.ffn(x))
        return x
```

#### è§£ç å™¨å±‚
```python
class DecoderLayer:
    def __init__(self):
        self.self_attn = MultiHeadAttention()
        self.cross_attn = MultiHeadAttention()
        self.ffn = FeedForward()
        self.norm1 = LayerNorm()
        self.norm2 = LayerNorm()
        self.norm3 = LayerNorm()
    
    def forward(self, x, encoder_output):
        # æ©ç è‡ªæ³¨æ„åŠ›
        x = self.norm1(x + self.self_attn(x, x, x, mask=True))
        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
        x = self.norm2(x + self.cross_attn(x, encoder_output, encoder_output))
        # å‰é¦ˆç½‘ç»œ
        x = self.norm3(x + self.ffn(x))
        return x
```

## é¢„è®­ç»ƒæŠ€æœ¯

### è¯­è¨€æ¨¡å‹ç›®æ ‡

#### å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal LMï¼‰
```python
# è‡ªå›å½’é¢„æµ‹
P(wt | w1, w2, ..., w(t-1)) = softmax(W Ã— ht)

æŸå¤±å‡½æ•°ï¼š
L = -Î£t log P(wt | w<t)

ç‰¹ç‚¹ï¼š
- ä»å·¦åˆ°å³ç”Ÿæˆ
- é€‚åˆæ–‡æœ¬ç”Ÿæˆ
- GPT ç³»åˆ—ä½¿ç”¨
```

#### æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked LMï¼‰
```python
# æ©ç é¢„æµ‹
P(wi | context) = softmax(W Ã— hi)

æŸå¤±å‡½æ•°ï¼š
L = -Î£iâˆˆmasked log P(wi | context)

ç‰¹ç‚¹ï¼š
- åŒå‘ä¸Šä¸‹æ–‡
- é€‚åˆç†è§£ä»»åŠ¡
- BERT ç³»åˆ—ä½¿ç”¨
```

#### åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰
```python
# ç¼–ç å™¨-è§£ç å™¨æ¶æ„
P(y | x) = Î t P(yt | y<t, x)

æŸå¤±å‡½æ•°ï¼š
L = -Î£t log P(yt | y<t, x)

ç‰¹ç‚¹ï¼š
- è¾“å…¥è¾“å‡ºä¸åŒé•¿åº¦
- é€‚åˆç¿»è¯‘ä»»åŠ¡
- T5ã€BART ä½¿ç”¨
```

### å¤§è§„æ¨¡è®­ç»ƒ

#### æ•°æ®å¤„ç†
```python
# æ•°æ®æ¸…æ´—
- å»é‡å¤„ç†
- è´¨é‡è¿‡æ»¤
- æ ¼å¼æ ‡å‡†åŒ–
- æ•æ„Ÿä¿¡æ¯å»é™¤

# åˆ†è¯ç­–ç•¥
- BPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰
- WordPiece
- SentencePiece
```

#### è®­ç»ƒç­–ç•¥
```python
# æ··åˆç²¾åº¦è®­ç»ƒ
with autocast():
    logits = model(inputs)
    loss = loss_fn(logits, targets)

# æ¢¯åº¦ç´¯ç§¯
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# å­¦ä¹ ç‡è°ƒåº¦
def lr_schedule(step):
    if step < warmup_steps:
        return lr_max * step / warmup_steps
    else:
        return lr_max * 0.5 * (1 + cos(Ï€ * (step - warmup_steps) / (total_steps - warmup_steps)))
```

## ç»å…¸æ¨¡å‹æ¶æ„

### GPT ç³»åˆ—

#### GPT-1ï¼ˆ2018ï¼‰
- **å‚æ•°é‡**ï¼š117M
- **æ¶æ„**ï¼š12å±‚è§£ç å™¨
- **è®­ç»ƒæ•°æ®**ï¼šBooksCorpus
- **åˆ›æ–°ç‚¹**ï¼šç”Ÿæˆå¼é¢„è®­ç»ƒ

#### GPT-2ï¼ˆ2019ï¼‰
- **å‚æ•°é‡**ï¼š1.5Bï¼ˆæœ€å¤§ï¼‰
- **æ¶æ„**ï¼š48å±‚è§£ç å™¨
- **è®­ç»ƒæ•°æ®**ï¼šWebTextï¼ˆ40GBï¼‰
- **åˆ›æ–°ç‚¹**ï¼šé›¶æ ·æœ¬èƒ½åŠ›

#### GPT-3ï¼ˆ2020ï¼‰
- **å‚æ•°é‡**ï¼š175Bï¼ˆæœ€å¤§ï¼‰
- **æ¶æ„**ï¼š96å±‚è§£ç å™¨
- **è®­ç»ƒæ•°æ®**ï¼šCommon Crawlï¼ˆ570GBï¼‰
- **åˆ›æ–°ç‚¹**ï¼šä¸Šä¸‹æ–‡å­¦ä¹ 

#### ChatGPT/GPT-4ï¼ˆ2022-2023ï¼‰
- **å‚æ•°é‡**ï¼šæœªå…¬å¼€ï¼ˆä¼°è®¡>1Tï¼‰
- **æ¶æ„**ï¼šå¤šæ¨¡æ€æ”¯æŒ
- **è®­ç»ƒæ•°æ®**ï¼šäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ 
- **åˆ›æ–°ç‚¹**ï¼šå¯¹è¯èƒ½åŠ›ã€å®‰å…¨æ€§

### BERT ç³»åˆ—

#### BERT-Baseï¼ˆ2018ï¼‰
- **å‚æ•°é‡**ï¼š110M
- **æ¶æ„**ï¼š12å±‚ç¼–ç å™¨
- **éšè—å±‚**ï¼š768ç»´
- **æ³¨æ„åŠ›å¤´**ï¼š12ä¸ª

#### BERT-Largeï¼ˆ2018ï¼‰
- **å‚æ•°é‡**ï¼š340M
- **æ¶æ„**ï¼š24å±‚ç¼–ç å™¨
- **éšè—å±‚**ï¼š1024ç»´
- **æ³¨æ„åŠ›å¤´**ï¼š16ä¸ª

#### RoBERTaï¼ˆ2019ï¼‰
- **æ”¹è¿›ç‚¹**ï¼š
  - æ›´é•¿è®­ç»ƒæ—¶é—´
  - æ›´å¤§æ•°æ®é›†
  - ç§»é™¤ä¸‹ä¸€å¥é¢„æµ‹
  - åŠ¨æ€æ©ç ç­–ç•¥

#### ALBERTï¼ˆ2019ï¼‰
- **æ”¹è¿›ç‚¹**ï¼š
  - å‚æ•°å…±äº«
  - å¥å­é¡ºåºé¢„æµ‹
  - å› å­åˆ†è§£åµŒå…¥

### T5 ç³»åˆ—

#### T5-Baseï¼ˆ2019ï¼‰
- **å‚æ•°é‡**ï¼š220M
- **æ¶æ„**ï¼š12å±‚ç¼–ç å™¨-è§£ç å™¨
- **ç»Ÿä¸€æ¡†æ¶**ï¼šText-to-Text
- **å¤šä»»åŠ¡é¢„è®­ç»ƒ**

#### T5-XXLï¼ˆ2019ï¼‰
- **å‚æ•°é‡**ï¼š11B
- **æ¶æ„**ï¼š24å±‚ç¼–ç å™¨-è§£ç å™¨
- **æ€§èƒ½**ï¼šå¤šä¸ªSOTAç»“æœ

## å¾®è°ƒæŠ€æœ¯

### å‚æ•°é«˜æ•ˆå¾®è°ƒ

#### LoRAï¼ˆä½ç§©é€‚åº”ï¼‰
```python
# ä½ç§©åˆ†è§£
W = W0 + BA

å…¶ä¸­ï¼š
- W0: é¢„è®­ç»ƒæƒé‡ï¼ˆå†»ç»“ï¼‰
- B: (d Ã— r) çŸ©é˜µ
- A: (r Ã— k) çŸ©é˜µ
- r << min(d, k): ä½ç§©ç»´åº¦

ç‰¹ç‚¹ï¼š
- å‚æ•°é‡å¤§å¹…å‡å°‘
- ä¿æŒé¢„è®­ç»ƒçŸ¥è¯†
- æ˜“äºéƒ¨ç½²å’Œåˆ‡æ¢
```

#### Prefix Tuning
```python
# å‰ç¼€ä¼˜åŒ–
åªä¼˜åŒ–è¾“å…¥å‰ç¼€å‚æ•°ï¼š
[PREFIX] x1 x2 ... xn [OUTPUT]

ç‰¹ç‚¹ï¼š
- ä¼˜åŒ–å°‘é‡å‰ç¼€token
- ä¿æŒæ¨¡å‹ä¸»ä½“ä¸å˜
- é€‚åº”ç”Ÿæˆä»»åŠ¡
```

#### Adapter
```python
# é€‚é…å™¨æ’å…¥
x â†’ LayerNorm â†’ Linear â†’ Nonlinear â†’ Linear â†’ LayerNorm â†’ y

ç‰¹ç‚¹ï¼š
- æ¯å±‚æ’å…¥å°å‹ç½‘ç»œ
- å‚æ•°é‡å¢åŠ å¾ˆå°‘
- ä¿æŒåŸæœ‰æ¶æ„
```

### æŒ‡ä»¤å¾®è°ƒ

#### æ•°æ®æ„é€ 
```python
# æŒ‡ä»¤æ ¼å¼
{
    "instruction": "ä»»åŠ¡æè¿°",
    "input": "è¾“å…¥å†…å®¹", 
    "output": "æœŸæœ›è¾“å‡º"
}

# å¤šæ ·æ€§ä¿è¯
- ä¸åŒä»»åŠ¡ç±»å‹
- ä¸åŒè¡¨è¾¾æ–¹å¼
- ä¸åŒéš¾åº¦çº§åˆ«
```

#### è®­ç»ƒç­–ç•¥
```python
# å¤šä»»åŠ¡å­¦ä¹ 
for batch in dataloader:
    tasks = batch['tasks']
    inputs = batch['inputs']
    targets = batch['targets']
    
    losses = []
    for task in tasks:
        loss = model(inputs[task], targets[task])
        losses.append(loss)
    
    total_loss = sum(losses) / len(losses)
    total_loss.backward()
```

## æ¨ç†ä¼˜åŒ–

### è§£ç ç­–ç•¥

#### è´ªå©ªæœç´¢
```python
# æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯
token = argmax(P(wt | w<t))

ç‰¹ç‚¹ï¼š
- é€Ÿåº¦å¿«
- ç¡®å®šæ€§è¾“å‡º
- å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜
```

#### æŸæœç´¢ï¼ˆBeam Searchï¼‰
```python
# ä¿æŒtop-kä¸ªå€™é€‰åºåˆ—
beams = [(sequence, score)]
for step in range(max_length):
    new_beams = []
    for seq, score in beams:
        next_tokens = get_top_k(seq, k)
        for token, prob in next_tokens:
            new_beams.append((seq + [token], score + log(prob)))
    beams = sorted(new_beams, key=lambda x: x[1])[:beam_size]
```

#### é‡‡æ ·æ–¹æ³•
```python
# æ¸©åº¦é‡‡æ ·
probs = softmax(logits / T)
token = sample(probs)

# Top-k é‡‡æ ·
probs = softmax(logits)
top_k_probs, top_k_indices = topk(probs, k)
token = sample(top_k_probs)

# Nucleus é‡‡æ ·
sorted_probs, sorted_indices = sort(probs)
cumsum_probs = cumsum(sorted_probs)
nucleus_indices = sorted_indices[cumsum_probs <= p]
```

### æ€§èƒ½ä¼˜åŒ–

#### æ¨¡å‹å‹ç¼©
```python
# é‡åŒ–
FP32 â†’ INT8: 4å€å‹ç¼©
FP32 â†’ FP16: 2å€å‹ç¼©

# å‰ªæ
importance_scores = calculate_importance(weights)
pruned_weights = prune(weights, importance_scores, ratio=0.9)

# è’¸é¦
teacher_logits = teacher_model(inputs)
student_logits = student_model(inputs)
loss = Î± * CE(student_logits, targets) + Î² * KL(student_logits, teacher_logits)
```

#### æ¨ç†åŠ é€Ÿ
```python
# KVç¼“å­˜
cache = {}
def generate_with_cache(input_ids):
    if input_ids not in cache:
        cache[input_ids] = model.get_kv_cache(input_ids)
    return model.generate_with_cache(input_ids, cache[input_ids])

# æ‰¹å¤„ç†æ¨ç†
batch_inputs = prepare_batch(inputs)
batch_outputs = model.generate_batch(batch_inputs)

# æµå¼æ¨ç†
for token in model.generate_stream(input):
    yield token
```

## åº”ç”¨åœºæ™¯

### æ–‡æœ¬ç”Ÿæˆ

#### å¯¹è¯ç³»ç»Ÿ
```python
# å¯¹è¯æ¨¡æ¿
prompt = f"""
ç³»ç»Ÿ: {system_prompt}
ç”¨æˆ·: {user_input}
åŠ©æ‰‹: """

# ç”Ÿæˆå›å¤
response = model.generate(prompt, max_length=512, temperature=0.7)
```

#### å†…å®¹åˆ›ä½œ
```python
# å†™ä½œè¾…åŠ©
prompt = f"""
ä¸»é¢˜: {topic}
é£æ ¼: {style}
é•¿åº¦: {length}

è¯·ç”Ÿæˆ{style}é£æ ¼çš„{topic}æ–‡ç« ï¼Œçº¦{length}å­—ï¼š
"""

# ä»£ç ç”Ÿæˆ
prompt = f"""
# {description}
def {function_name}():
    # è¯·å®ç°è¿™ä¸ªå‡½æ•°
"""
```

### æ–‡æœ¬ç†è§£

#### åˆ†ç±»ä»»åŠ¡
```python
# æƒ…æ„Ÿåˆ†æ
prompt = f"""
æ–‡æœ¬: {text}
æƒ…æ„Ÿ: [æ­£é¢/è´Ÿé¢/ä¸­æ€§]

è¯·åˆ†æä¸Šè¿°æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼š
"""

# æ„å›¾è¯†åˆ«
prompt = f"""
ç”¨æˆ·è¾“å…¥: {user_input}
æ„å›¾: [æŸ¥è¯¢/é¢„è®¢/æŠ•è¯‰/å…¶ä»–]

è¯·è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼š
"""
```

#### ä¿¡æ¯æŠ½å–
```python
# å‘½åå®ä½“è¯†åˆ«
prompt = f"""
æ–‡æœ¬: {text}
å®ä½“: [äººå/åœ°å/æœºæ„å/æ—¶é—´]

è¯·æå–æ–‡æœ¬ä¸­çš„å‘½åå®ä½“ï¼š
"""

# å…³ç³»æŠ½å–
prompt = f"""
æ–‡æœ¬: {text}
å…³ç³»ç±»å‹: [é›‡ä½£/åˆ›å§‹/ä½äº/å…¶ä»–]

è¯·æå–å®ä½“é—´çš„å…³ç³»ï¼š
"""
```

## è¯„ä¼°æ–¹æ³•

### è‡ªåŠ¨è¯„ä¼°

#### å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
```python
# è®¡ç®—å›°æƒ‘åº¦
perplexity = exp(-1/N * Î£i log P(wi | context))

ç‰¹ç‚¹ï¼š
- è¡¡é‡è¯­è¨€æ¨¡å‹è´¨é‡
- å€¼è¶Šå°è¶Šå¥½
- è¯„ä¼°ç”Ÿæˆè´¨é‡
```

#### BLEU åˆ†æ•°
```python
# BLEU è®¡ç®—
BLEU = BP Ã— exp(Î£n wn log pn)

å…¶ä¸­ï¼š
- BP: é•¿åº¦æƒ©ç½šå› å­
- pn: n-gram ç²¾ç¡®åº¦
- wn: æƒé‡ï¼ˆé€šå¸¸ä¸º1/nï¼‰
```

#### ROUGE åˆ†æ•°
```python
# ROUGE-Lï¼ˆæœ€é•¿å…¬å…±å­åºåˆ—ï¼‰
ROUGE-L = LCS(reference, candidate) / len(reference)

# ROUGE-Nï¼ˆn-gramå¬å›ï¼‰
ROUGE-N = match_ngrams / total_ngrams_reference
```

### äººå·¥è¯„ä¼°

#### è¯„ä¼°ç»´åº¦
- **æµç•…æ€§**ï¼šè¯­è¨€æ˜¯å¦è‡ªç„¶æµç•…
- **è¿è´¯æ€§**ï¼šå†…å®¹æ˜¯å¦é€»è¾‘è¿è´¯
- **ç›¸å…³æ€§**ï¼šæ˜¯å¦å›ç­”äº†é—®é¢˜
- **å‡†ç¡®æ€§**ï¼šä¿¡æ¯æ˜¯å¦å‡†ç¡®æ— è¯¯
- **å®‰å…¨æ€§**ï¼šæ˜¯å¦åŒ…å«æœ‰å®³å†…å®¹

#### è¯„ä¼°æ–¹æ³•
- **ç»å¯¹è¯„åˆ†**ï¼š1-5åˆ†ç›´æ¥è¯„åˆ†
- **ç›¸å¯¹æ¯”è¾ƒ**ï¼šä¸¤ä¸ªç‰ˆæœ¬å¯¹æ¯”
- **æ’åºè¯„ä¼°**ï¼šå¤šä¸ªç‰ˆæœ¬æ’åº
- **A/Bæµ‹è¯•**ï¼šç”¨æˆ·åå¥½æµ‹è¯•

## å‘å±•è¶‹åŠ¿

### æ¶æ„åˆ›æ–°

#### ç¨€ç–æ³¨æ„åŠ›
```python
# å±€éƒ¨æ³¨æ„åŠ›
åªå…³æ³¨é‚»è¿‘tokençš„æ³¨æ„åŠ›

# å…¨å±€æ³¨æ„åŠ›  
éƒ¨åˆ†tokenå…³æ³¨å…¨å±€ï¼Œå…¶ä»–å…³æ³¨å±€éƒ¨

# éšæœºæ³¨æ„åŠ›
éšæœºé€‰æ‹©æ³¨æ„åŠ›æ¨¡å¼
```

#### æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰
```python
# ä¸“å®¶è·¯ç”±
router_output = router(input)
expert_weights = softmax(router_output)
final_output = Î£ expert_weights[i] Ã— expert_i(input)

ç‰¹ç‚¹ï¼š
- å‚æ•°é‡å·¨å¤§
- è®¡ç®—é‡å¯æ§
- ä¸“ä¸šåŒ–åˆ†å·¥
```

### è®­ç»ƒæŠ€æœ¯

#### è‡ªç›‘ç£å­¦ä¹ 
- **å¯¹æ¯”å­¦ä¹ **ï¼šSimCLRã€MoCo
- **æ©ç å»ºæ¨¡**ï¼šMAEã€BEiT
- **ç”Ÿæˆå¼é¢„è®­ç»ƒ**ï¼šGPTé£æ ¼

#### å¤šæ¨¡æ€å­¦ä¹ 
- **è§†è§‰-è¯­è¨€**ï¼šCLIPã€ALIGN
- **éŸ³é¢‘-è¯­è¨€**ï¼šWhisper
- **å¤šæ¨¡æ€ç»Ÿä¸€**ï¼šFlamingo

### åº”ç”¨æ–¹å‘

#### ä¸ªæ€§åŒ–
- **ç”¨æˆ·é€‚é…**ï¼šå¾®è°ƒé€‚åº”ç‰¹å®šç”¨æˆ·
- **é¢†åŸŸé€‚é…**ï¼šä¸“ä¸šé¢†åŸŸä¼˜åŒ–
- **é£æ ¼é€‚é…**ï¼šå†™ä½œé£æ ¼å­¦ä¹ 

#### å¯æ§æ€§
- **å±æ€§æ§åˆ¶**ï¼šæƒ…æ„Ÿã€ä¸»é¢˜ã€é•¿åº¦
- **å†…å®¹è¿‡æ»¤**ï¼šå®‰å…¨ã€åˆè§„æ€§
- **äº‹å®æ€§**ï¼šå‡å°‘å¹»è§‰

---

**å­¦ä¹ è·¯å¾„å®Œæˆï¼** ğŸ‰

æ¥ä¸‹æ¥å¯ä»¥å­¦ä¹ ï¼š
- [å¤§æ¨¡å‹åº”ç”¨å¼€å‘](../app-development/01-introduction.md)
- [å¤§æ¨¡å‹åŸç†æ·±å…¥](../llm-fundamentals/01-introduction.md)
